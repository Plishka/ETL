{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78e79ec6",
   "metadata": {},
   "source": [
    "## Getting data (points) from Scroll Web_Site API in batches saving progress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1376c3",
   "metadata": {},
   "source": [
    "### Key Points in the Code:\n",
    " - **Checkpoint File:** Before starting, the script checks for a checkpoint file (**wallets_with_points_checkpoint.csv**) to resume from where it left off.\n",
    " - **Batch Processing:** Wallet addresses are processed in batches. After processing each batch, the results are appended to the df_results DataFrame.\n",
    " - **Periodic Saving:** After processing each batch, the results are saved to the checkpoint file to ensure progress is not lost.\n",
    " - **Resume Capability:** If interrupted, the script can resume from the last processed batch by reading the checkpoint file.\n",
    "\n",
    "This approach minimizes data loss and allows to handle large datasets by breaking the task into smaller, manageable batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c5a3c4",
   "metadata": {},
   "source": [
    "## Getting Data using parallel requests to speed up the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cf75ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "from datetime import datetime\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02819efc",
   "metadata": {},
   "source": [
    "### Function to fetch points from AWS API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61c990f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch points for a wallet\n",
    "def fetch_points(wallet):\n",
    "    try:\n",
    "        response = requests.get(f'https://kx58j6x5me.execute-api.us-east-1.amazonaws.com/scroll/wallet-points?walletAddress={wallet}')\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if isinstance(data, list) and len(data) > 0:\n",
    "                return data[0].get('points')\n",
    "            elif isinstance(data, dict):\n",
    "                return data.get('points')\n",
    "        else:\n",
    "            print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}: Error: Received status code {response.status_code} for wallet {wallet}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}: Error: Request failed for wallet {wallet}. Exception: {e}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c53522d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load wallet addresses\n",
    "wallet_addresses = pd.read_csv(\"scrollwallets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55d96f5",
   "metadata": {},
   "source": [
    "### Create Checkpoint File to allow save data extraction progress and resume from the inrerruption point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c35ac9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-31 17:05:56: Resuming from checkpoint. Processed 348000 wallets so far.\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoint file if it exists\n",
    "checkpoint_file = \"wallets_with_points_checkpoint.csv\" # assign name to search for our chekpoint file\n",
    "\n",
    "if os.path.exists(checkpoint_file):\n",
    "    df_checkpoint = pd.read_csv(checkpoint_file)\n",
    "    processed_wallets = set(df_checkpoint['Wallet'].tolist()) # transform list to SET to ensure each wallet address is unique\n",
    "    start_index = len(processed_wallets) \n",
    "    df_results = df_checkpoint.copy() # Assign copy to ensure that any further operations on df_results do not affect the original checkpoint DataFrame\n",
    "    print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}: Resuming from checkpoint. Processed {start_index} wallets so far.\")\n",
    "else: # Create empty template for checkpoint file (if it does not exist)\n",
    "    processed_wallets = set() # create enpty SET\n",
    "    start_index = 0\n",
    "    df_results = pd.DataFrame(columns=['Wallet', 'Points']) \n",
    "    print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}: No checkpoint found. Starting from the beginning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c62066",
   "metadata": {},
   "source": [
    "### Function to process a batch of wallets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "651ccb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Size specifies the number of requests in one cycle. After processing the progress will be saved\n",
    "batch_size = 1000\n",
    "\n",
    "# num_cores = multiprocessing.cpu_count() # Gets the number of PC Cores to automaticaly set optimal streams\n",
    "# max_workers = num_cores * 2  # Adjust this multiplier based on experimentation not to overload CPU\n",
    "\n",
    "# Function to process a batch of wallets\n",
    "def process_batch(batch):\n",
    "    results = [] # to store points of our batch\n",
    "    # Function allows to make parallel requests to speed up the process. \n",
    "    with ThreadPoolExecutor(max_workers=8) as executor: # Specify amount of streams - max_workers\n",
    "        futures = {executor.submit(fetch_points, row['wallet']): row['wallet'] for index, row in batch.iterrows()} # assign tasks to executors\n",
    "        for future in as_completed(futures):\n",
    "            wallet = futures[future]\n",
    "            try:\n",
    "                points = future.result()\n",
    "                results.append({'Wallet': wallet, 'Points': points})\n",
    "            except Exception as e:\n",
    "                print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}: Exception for wallet {wallet}: {e}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce8ba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process wallet addresses in batches\n",
    "for i in range(start_index, len(wallet_addresses), batch_size):\n",
    "    batch = wallet_addresses.iloc[i:i + batch_size]\n",
    "    batch = batch[~batch['wallet'].isin(processed_wallets)]  # Exclude already processed wallets\n",
    "    batch_results = process_batch(batch)\n",
    "    df_batch = pd.DataFrame(batch_results)\n",
    "    df_results = pd.concat([df_results, df_batch], ignore_index=True)\n",
    "    df_results['Points'] = df_results['Points'].apply(lambda x: round(x, 3) if x is not None else x)\n",
    "    df_results.to_csv(checkpoint_file, index=False)\n",
    "    processed_wallets.update(batch['wallet'].tolist())\n",
    "    print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}: Processed up to index {i + batch_size} out of {len(wallet_addresses)}\")\n",
    "    time.sleep(0.4)  # Adjust sleep time to avoid hitting rate limits\n",
    "\n",
    "# Save the final results to a CSV file\n",
    "df_results.to_csv(\"wallets_with_points.csv\", index=False)\n",
    "print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}: All data processed and saved to wallets_with_points.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
